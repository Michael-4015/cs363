{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name(s): John Michael Tran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Association Analysis\n",
    "\n",
    "Association analysis uses machine learning to extract frequent itemsets and strong association rules from large datasets. In this assignment you'll be implementing one of the most commonly used algorithms for association rule mining - the Apriori algorithm.\n",
    "\n",
    "The dataset (`large_retail.txt`) that we are going to use has been adapted from the [Retail Market Basket Dataset](http://fimi.ua.ac.be/data/retail.pdf). This dataset contains transaction records supplied by a Belgian retail supermarket store. Each line in the file represents a separate transaction with the item ids separated by space. The dataset has 3000 transactions and 99 different item ids.\n",
    "\n",
    "You are also provided with a smaller dataset (`small_retail.txt`) with 9 transactions and 5 different item ids along with the solutions. You can test and debug your implementation on this smaller dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apriori Algorithm from scratch\n",
    "\n",
    "The Apriori algorithm is a classical algorithm in data mining. It is used for mining frequent itemsets and relevant association rules. In this part, you'll be implementing this algorithm for generating the itemsets that occur enough times to meet the `min_sup` threshold.\n",
    "\n",
    "**Implementation Hint:**\n",
    "\n",
    "- Use the `frozenset` data structure in Python, which is similar to `set` in functionality, to represent the itemsets, because `frozenset` is an immutable (hashable) data structure. You can maintain a dictionary that maps from the itemset (a `frozenset`) to its support count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports (you can add additional headers if you wish)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset from file\n",
    "def load_dataset(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        content = f.readlines()\n",
    "        data = [[int(x) for x in line.rstrip().split()] for line in content]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 5],\n",
       " [2, 4],\n",
       " [2, 3],\n",
       " [1, 2, 4],\n",
       " [1, 3],\n",
       " [2, 3],\n",
       " [1, 3],\n",
       " [1, 2, 3, 5],\n",
       " [1, 2, 3]]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the small_retail dataset\n",
    "small_dataset = load_dataset('small_retail.txt')\n",
    "small_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.** Implement the function `create_1_itemsets` that takes as input the entire dataset and returns a list of all the 1-itemsets. For example, for `small_retail.txt` it should return:\n",
    "~~~\n",
    "[frozenset({1}),\n",
    " frozenset({2}),\n",
    " frozenset({3}),\n",
    " frozenset({4}),\n",
    " frozenset({5})]\n",
    " ~~~\n",
    " Please **don't hardcode** the item ids, your code should support item ids that are non-sequential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_1_itemsets(dataset):\n",
    "    c1 = []\n",
    "    for dp in dataset :\n",
    "        for item in dp :\n",
    "            one_item = frozenset([item])\n",
    "            if one_item not in c1 :\n",
    "                c1.append(one_item)\n",
    "    return c1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.** Implement function `filter_candidates` that takes as input the candidate itemsets, the dataset, and the minumum support count `min_sup`, and filters out candidates that don't meet the support threshold.\n",
    "\n",
    "You should also return the support count information as a `dict` for the itemsets that meet `min_sup`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_candidates(candidates, dataset, min_sup):\n",
    "    retlist = []\n",
    "    support_data = {}\n",
    "    counts = {}\n",
    "    for transaction in dataset :\n",
    "        for itemset in candidates :\n",
    "            if itemset.issubset(transaction) :\n",
    "                if itemset in counts :\n",
    "                    counts[itemset] += 1\n",
    "                else :\n",
    "                    counts[itemset] = 1\n",
    "    \n",
    "    for itemset in counts :\n",
    "        if counts[itemset] >= min_sup :\n",
    "            retlist.append(itemset)\n",
    "            support_data[itemset] = counts[itemset]\n",
    " \n",
    "\n",
    "    sortlist = [list(sets) for sets in retlist]\n",
    "    sortlist.sort()\n",
    "    retlist = [frozenset(sort) for sort in sortlist]\n",
    "    return retlist, support_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.** Implement the function `generate_next_itemsets` that takes in frequent itemsets of size `k` and generates candidate itemsets of size `k + 1`.\n",
    "\n",
    "Use either the F(k-1) x F(k-1) or the F(k-1) x F(1) candidate generation method, then **filter the candidate list based on the apriori principle before returning it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_itemsets(freq_sets):\n",
    "    retList = []\n",
    "    for i in range(len(freq_sets)) :\n",
    "        for j in range(i+1, len(freq_sets)) :\n",
    "            curr_set = freq_sets[i]\n",
    "            next_set = freq_sets[j]\n",
    "            curr_list = list(curr_set)\n",
    "            next_list = list(next_set)\n",
    "            curr_list.sort()\n",
    "            next_list.sort()\n",
    "            if curr_list[:-1]  == next_list[:-1] : \n",
    "                merge_set = curr_set.union(next_set)\n",
    "                retList.append(merge_set)\n",
    "\n",
    "    return retList\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.** Implement the function `apriori_freq_itemsets` that takes the entire dataset as input and returns all the frequent itemsets that meet `min_sup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apriori_freq_itemsets(dataset, minsup):\n",
    "    c1 = create_1_itemsets(dataset)\n",
    "    freq_list, freq_counts = filter_candidates(c1, dataset, minsup)\n",
    "    retlist = [freq_list]\n",
    "    index = 0\n",
    "    while len(retlist[index]) > 0 :\n",
    "        itemset = retlist[index]\n",
    "        n_itemset = generate_next_itemsets(itemset)\n",
    "        n_freq_list, n_freq_counts = filter_candidates(n_itemset, dataset, minsup)\n",
    "        retlist.append(n_freq_list)\n",
    "        freq_counts.update(n_freq_counts)\n",
    "        index+=1\n",
    "    return retlist, freq_counts\n",
    "\n",
    "def print_apriori_table(items, freq_counts,dataset) : \n",
    "    print(\"Sup\\tFreq Itemset\")\n",
    "    for item in items :\n",
    "        for i in item :\n",
    "            print(str(round(freq_counts[i] / len(dataset),2)) + \"\\t\" + str(list(i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5.** Display the frequent item sets in the form of a table along with their `support` (as a fraction: support count over number of transactions) for the `large_retail.txt` dataset **with a min support count of 300**.\n",
    "\n",
    "Sample Table Format (tab separated table)\n",
    "\n",
    "~~~\n",
    "Sup     Freq Itemset\n",
    "0.67\t[1]\n",
    "0.44\t[1, 2]\n",
    "(and so on)\n",
    "...\n",
    "...\n",
    "~~~\n",
    "\n",
    "`support(itemset) = support_count(itemset) / num_total_transactions`.\n",
    "\n",
    "The `support` and the itemset should be separated by a tab (`'\\t'`).\n",
    "\n",
    "Note that the `support` should be rounded to the nearest 2 decimal places (use `round(sup, 2)`). If a support_fraction only contains 1 decimal place (for example, 0.1), you do not need to add a 0 to the end of it (leaving it as 0.1 is fine).\n",
    "\n",
    "The itemsets should also be in a sorted order where smaller itemsets should come before larger itemsets and itemsets of the same size should be sorted amongst themselves.\n",
    "\n",
    "For eg. \n",
    "~~~~\n",
    "[1, 2] should come before [1, 2, 3]\n",
    "[1, 2, 3] should come before [1, 2, 4]\n",
    "[1, 2, 3] should come before [1, 4, 5]\n",
    "[1, 2, 3] should come before [2, 3, 4]\n",
    "~~~~\n",
    "\n",
    "Note that **this order is very important for grading!** \n",
    "\n",
    "The output also **shouldn't contain any duplicates**. \n",
    "\n",
    "The sample output for the `small_retail.txt` dataset with `min_sup` set to 2 is:\n",
    "\n",
    "~~~~\n",
    "Sup     Freq Itemset\n",
    "0.67    [1]\n",
    "0.78\t[2]\n",
    "0.67\t[3]\n",
    "0.22\t[4]\n",
    "0.22\t[5]\n",
    "0.44\t[1, 2]\n",
    "0.44\t[1, 3]\n",
    "0.22\t[1, 5]\n",
    "0.44\t[2, 3]\n",
    "0.22\t[2, 4]\n",
    "0.22\t[2, 5]\n",
    "0.22\t[1, 2, 3]\n",
    "0.22\t[1, 2, 5]\n",
    "~~~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sup\tFreq Itemset\n",
      "0.1\t[31]\n",
      "0.14\t[32]\n",
      "0.11\t[36]\n",
      "0.26\t[38]\n",
      "0.53\t[39]\n",
      "0.22\t[41]\n",
      "0.47\t[48]\n",
      "0.11\t[60]\n",
      "0.11\t[65]\n",
      "0.11\t[89]\n",
      "0.14\t[32, 39]\n",
      "0.15\t[38, 39]\n",
      "0.14\t[41, 39]\n",
      "0.13\t[48, 38]\n",
      "0.33\t[48, 39]\n",
      "0.18\t[48, 41]\n",
      "0.14\t[48, 41, 39]\n"
     ]
    }
   ],
   "source": [
    "large_dataset = load_dataset('large_retail.txt')\n",
    "items, freq_counts = apriori_freq_itemsets(large_dataset, 300)\n",
    "print_apriori_table(items, freq_counts, large_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
